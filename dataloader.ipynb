{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scribe: Realistic Handwriting with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A deep learning project by Sam Greydanus. August 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook simply explores how to load the handwriting dataset in preparation for training the model on real handwriting data.\n",
    "\n",
    "First time use: download dataset from [IAM Handwriting Database](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database). There are two folders in this dataset that matter: 'ascii' and 'lineStrokes'. Put these in a './data' directory relative to this notebook. When an instance of this model is created for the first time it will parse all of the xml data in these files and save a processed version to a pickle file. This takes about 10 minutes but you only need to do it once\n",
    "\n",
    "I took some of the code for this class from **hardmaru**'s [write-rnn-tensorflow](https://github.com/hardmaru/write-rnn-tensorflow) project and modified it to return the ascii labels in addition to the pen stroke data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "import os\nimport pickle\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport random\nfrom utils import *\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data loader class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is almost identical to the one I used to train the handwriting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "class DataLoader():\n    def __init__(self, batch_size=50, tsteps=300, scale_factor = 10, U_items=10, limit = 500, alphabet=\"default\"):\n        self.data_dir = \"./data\"\n        self.alphabet = alphabet\n        self.batch_size = batch_size\n        self.tsteps = tsteps\n        self.scale_factor = scale_factor # divide data by this factor\n        self.limit = limit # removes large noisy gaps in the data\n        self.U_items = U_items\n\n        data_file = os.path.join(self.data_dir, \"strokes_training_data.cpkl\")\n        stroke_dir = self.data_dir+\"/lineStrokes\"\n        ascii_dir = self.data_dir+\"/ascii\"\n\n        if not (os.path.exists(data_file)) :\n            print(\"creating training data cpkl file from raw source\")\n            self.preprocess(stroke_dir, ascii_dir, data_file)\n\n        self.load_preprocessed(data_file)\n        self.reset_batch_pointer()\n\n    def preprocess(self, stroke_dir, ascii_dir, data_file):\n        # create data file from raw xml files from iam handwriting source.\n        print(\"Parsing dataset...\")\n        \n        # build the list of xml files\n        filelist = []\n        # Set the directory you want to start from\n        rootDir = stroke_dir\n        for dirName, subdirList, fileList in os.walk(rootDir):\n#             print('Found directory: %s' % dirName)\n            for fname in fileList:\n#                 print('\\t%s' % fname)\n                filelist.append(dirName+\"/\"+fname)\n\n        # function to read each individual xml file\n        def getStrokes(filename):\n            tree = ET.parse(filename)\n            root = tree.getroot()\n\n            result = []\n\n            x_offset = 1e20\n            y_offset = 1e20\n            y_height = 0\n            for i in range(1, 4):\n                x_offset = min(x_offset, float(root[0][i].attrib['x']))\n                y_offset = min(y_offset, float(root[0][i].attrib['y']))\n                y_height = max(y_height, float(root[0][i].attrib['y']))\n            y_height -= y_offset\n            x_offset -= 100\n            y_offset -= 100\n\n            for stroke in root[1].findall('Stroke'):\n                points = []\n                for point in stroke.findall('Point'):\n                    points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n                result.append(points)\n            return result\n        \n        # function to read each individual xml file\n        def getAscii(filename, line_number):\n            with open(filename, \"r\") as f:\n                s = f.read()\n            s = s[s.find(\"CSR\"):]\n            if len(s.split(\"\\n\")) > line_number+2:\n                s = s.split(\"\\n\")[line_number+2]\n                return s\n            else:\n                return \"\"\n                \n        # converts a list of arrays into a 2d numpy int16 array\n        def convert_stroke_to_array(stroke):\n            n_point = 0\n            for i in range(len(stroke)):\n                n_point += len(stroke[i])\n            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n\n            prev_x = 0\n            prev_y = 0\n            counter = 0\n\n            for j in range(len(stroke)):\n                for k in range(len(stroke[j])):\n                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n                    prev_x = int(stroke[j][k][0])\n                    prev_y = int(stroke[j][k][1])\n                    stroke_data[counter, 2] = 0\n                    if (k == (len(stroke[j])-1)): # end of stroke\n                        stroke_data[counter, 2] = 1\n                    counter += 1\n            return stroke_data\n\n        # build stroke database of every xml file inside iam database\n        strokes = []\n        asciis = []\n        for i in range(len(filelist)):\n            if (filelist[i][-3:] == 'xml'):\n                stroke_file = filelist[i]\n#                 print('processing '+stroke_file)\n                stroke = convert_stroke_to_array(getStrokes(stroke_file))\n                \n                ascii_file = stroke_file.replace(\"lineStrokes\",\"ascii\")[:-7] + \".txt\"\n                line_number = stroke_file[-6:-4]\n                line_number = int(line_number) - 1\n                ascii = getAscii(ascii_file, line_number)\n                if len(ascii) > 10:\n                    strokes.append(stroke)\n                    asciis.append(ascii)\n                else:\n                    print(\"======>>>> Line length was too short. Line was: \" + ascii)\n                \n        assert(len(strokes)==len(asciis)), \"There should be a 1:1 correspondence between stroke data and ascii labels.\"\n        f = open(data_file,\"wb\")\n        pickle.dump([strokes,asciis], f, protocol=2)\n        f.close()\n        print(\"Finished parsing dataset. Saved {} lines\".format(len(strokes)))\n\n\n    def load_preprocessed(self, data_file):\n        f = open(data_file,\"rb\")\n        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n        f.close()\n\n        # goes thru the list, and only keeps the text entries that have more than tsteps points\n        self.stroke_data = []\n        self.ascii_data = []\n        counter = 0\n\n        for i in range(len(self.raw_stroke_data)):\n            data = self.raw_stroke_data[i]\n            if len(data) > (self.tsteps+2):\n                # removes large gaps from the data\n                data = np.minimum(data, self.limit)\n                data = np.maximum(data, -self.limit)\n                data = np.array(data,dtype=np.float32)\n                data[:,0:2] /= self.scale_factor\n                \n                self.stroke_data.append(data)\n                self.ascii_data.append(self.raw_ascii_data[i])\n\n        # minus 1, since we want the ydata to be a shifted version of x data\n        self.num_batches = int(len(self.stroke_data) / self.batch_size)\n        print(\"Loaded dataset:\")\n        print(\"   -> {} individual data points\".format(len(self.stroke_data)))\n        print(\"   -> {} batches\".format(self.num_batches))\n\n    def next_batch(self):\n        # returns a randomised, tsteps sized portion of the training data\n        x_batch = []\n        y_batch = []\n        ascii_list = []\n        for i in range(self.batch_size):\n            data = self.stroke_data[self.idx_perm[self.pointer]]\n            x_batch.append(np.copy(data[:self.tsteps]))\n            y_batch.append(np.copy(data[1:self.tsteps+1]))\n            ascii_list.append(self.ascii_data[self.idx_perm[self.pointer]])\n            self.tick_batch_pointer()\n        one_hots = [self.one_hot(s) for s in ascii_list]\n        return x_batch, y_batch, ascii_list, one_hots\n    \n    def one_hot(self, s):\n        #index position 0 means \"unknown\"\n        if self.alphabet == \"default\":\n            alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n        seq = [alphabet.find(char) + 1 for char in s]\n        if len(seq) >= self.U_items:\n            seq = seq[:self.U_items]\n        else:\n            seq = seq + [0]*(self.U_items - len(seq))\n        one_hot = np.zeros((self.U_items,len(alphabet)+1))\n        one_hot[np.arange(self.U_items),seq] = 1\n        return one_hot\n\n    def tick_batch_pointer(self):\n        self.pointer += 1\n        if (self.pointer >= len(self.stroke_data)):\n            self.reset_batch_pointer()\n    def reset_batch_pointer(self):\n        self.idx_perm = np.random.permutation(len(self.stroke_data))\n        self.pointer = 0\n        print(\"pointer reset\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "batch_size = 5\ntsteps = 700\ndata_scale = 50\nU_items = tsteps // 20\ndata_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, \\\n                         scale_factor=data_scale, U_items=U_items, alphabet=\"default\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_plot(strokes, title):\n",
    "    plt.figure(figsize=(20,2))\n",
    "    eos_preds = np.where(strokes[:,-1] == 1)\n",
    "    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n",
    "    for i in range(len(eos_preds)-1):\n",
    "        start = eos_preds[i]+1\n",
    "        stop = eos_preds[i+1]\n",
    "        plt.plot(strokes[start:stop,0], strokes[start:stop,1],'b-', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "x, y, s, c = data_loader.next_batch()\nprint(data_loader.pointer)\nfor i in range(batch_size):\n    r = x[i]\n    strokes = r.copy()\n    strokes[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n    line_plot(strokes, s[i][:U_items])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stroke styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the five handwriting styles from the blog post. In the future, I will use them to \"prime\" my model so that it will synthesize handwriting in a particular style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "with open(os.path.join('data', 'styles.p'),'rb') as f:\n    style_strokes, style_strings = pickle.load(f)\n    \nfor i in range(len(style_strokes)):\n    strokes = style_strokes[i]\n    strokes[:,:-1] = np.cumsum(strokes[:,:-1], axis=0)\n    line_plot(strokes, \"Style #{}: {}\".format(i+1, style_strings[i]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}