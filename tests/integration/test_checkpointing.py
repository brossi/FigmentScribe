"""
Integration Tests for Model Checkpointing

Tests checkpoint save/load functionality:
- Save checkpoint
- Load checkpoint
- Restore model state correctly
- Resume training from checkpoint
- CheckpointManager functionality

Usage:
    pytest tests/integration/test_checkpointing.py -v
    pytest -m integration -k checkpoint
"""

import pytest
import tensorflow as tf
import numpy as np
import os
from pathlib import Path


@pytest.mark.integration
class TestCheckpointSave:
    """Test checkpoint saving functionality."""

    def test_checkpoint_creates_files(self, tiny_model, temp_checkpoint_dir):
        """Test checkpoint saves files to disk."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        # Should create checkpoint files
        assert os.path.exists(save_path + '.index'), \
            "Checkpoint index file should exist"

    def test_checkpoint_manager_saves(self, tiny_model, temp_checkpoint_dir):
        """Test CheckpointManager saves checkpoints."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=5
        )

        # Save checkpoint
        save_path = manager.save(checkpoint_number=1)

        # Should have saved
        assert save_path is not None
        assert manager.latest_checkpoint is not None

    def test_checkpoint_with_optimizer(self, tiny_model, temp_checkpoint_dir):
        """Test checkpoint saves both model and optimizer."""
        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

        checkpoint = tf.train.Checkpoint(
            model=tiny_model,
            optimizer=optimizer
        )

        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        # Should create checkpoint files
        assert os.path.exists(save_path + '.index')

    def test_checkpoint_manager_max_to_keep(self, tiny_model, temp_checkpoint_dir):
        """Test CheckpointManager keeps only N most recent checkpoints."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=3
        )

        # Save 5 checkpoints
        for i in range(5):
            manager.save(checkpoint_number=i)

        # Should only keep 3 most recent
        checkpoints = manager.checkpoints
        assert len(checkpoints) == 3, \
            f"Should keep only 3 checkpoints, found {len(checkpoints)}"


@pytest.mark.integration
class TestCheckpointRestore:
    """Test checkpoint restoration functionality."""

    def test_checkpoint_restores_weights(self, tiny_model, temp_checkpoint_dir,
                                        reset_random_seeds):
        """Test checkpoint restores model weights correctly."""
        # Save initial weights
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        initial_weights = [var.numpy().copy() for var in tiny_model.trainable_variables]

        # Modify weights
        for var in tiny_model.trainable_variables:
            var.assign(tf.random.normal(var.shape))

        # Verify weights changed
        for initial, var in zip(initial_weights, tiny_model.trainable_variables):
            assert not np.allclose(initial, var.numpy()), \
                "Weights should have changed"

        # Restore from checkpoint
        checkpoint.restore(save_path)

        # Weights should match initial
        for initial, var in zip(initial_weights, tiny_model.trainable_variables):
            np.testing.assert_allclose(
                initial, var.numpy(),
                err_msg="Restored weights should match saved weights"
            )

    def test_checkpoint_restores_optimizer_state(self, tiny_model,
                                                 temp_checkpoint_dir,
                                                 sample_batch_inputs,
                                                 reset_random_seeds):
        """Test checkpoint restores optimizer state."""
        from model import compute_loss

        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

        # Train for one step to initialize optimizer state
        with tf.GradientTape() as tape:
            predictions = tiny_model(sample_batch_inputs, training=True)
            loss = compute_loss(predictions, sample_batch_inputs['stroke_data'])
        gradients = tape.gradient(loss, tiny_model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, tiny_model.trainable_variables))

        # Save checkpoint
        checkpoint = tf.train.Checkpoint(model=tiny_model, optimizer=optimizer)
        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        # Save optimizer state
        saved_lr = optimizer.learning_rate.numpy()

        # Create new optimizer and model
        new_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)

        # Restore checkpoint
        new_checkpoint = tf.train.Checkpoint(model=tiny_model, optimizer=new_optimizer)
        new_checkpoint.restore(save_path)

        # Learning rate should be restored
        restored_lr = new_optimizer.learning_rate.numpy()
        assert restored_lr == saved_lr, \
            f"Learning rate should be restored (saved={saved_lr}, restored={restored_lr})"

    def test_checkpoint_manager_latest_checkpoint(self, tiny_model,
                                                  temp_checkpoint_dir):
        """Test CheckpointManager finds latest checkpoint."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=3
        )

        # No checkpoint initially
        assert manager.latest_checkpoint is None

        # Save checkpoints
        for i in range(3):
            manager.save(checkpoint_number=i)

        # Should find latest
        assert manager.latest_checkpoint is not None
        assert 'ckpt-2' in manager.latest_checkpoint

    def test_restore_from_latest_checkpoint(self, tiny_model, temp_checkpoint_dir,
                                           reset_random_seeds):
        """Test restore from latest checkpoint works."""
        # Save multiple checkpoints
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=5
        )

        # Save initial state
        initial_weights = [var.numpy().copy() for var in tiny_model.trainable_variables]
        manager.save(checkpoint_number=0)

        # Modify weights and save again
        for var in tiny_model.trainable_variables:
            var.assign(tf.random.normal(var.shape))
        final_weights = [var.numpy().copy() for var in tiny_model.trainable_variables]
        manager.save(checkpoint_number=1)

        # Randomize weights
        for var in tiny_model.trainable_variables:
            var.assign(tf.zeros(var.shape))

        # Restore from latest
        checkpoint.restore(manager.latest_checkpoint)

        # Should match final weights (checkpoint 1, not 0)
        for final, var in zip(final_weights, tiny_model.trainable_variables):
            np.testing.assert_allclose(final, var.numpy())


@pytest.mark.integration
@pytest.mark.slow
class TestResumeTraining:
    """Test resuming training from checkpoint."""

    def test_resume_training_continues_from_checkpoint(self, tiny_model, mock_args,
                                                       mini_dataset_path,
                                                       temp_checkpoint_dir,
                                                       reset_random_seeds):
        """Test training can resume from saved checkpoint."""
        from model import compute_loss
        from utils import DataLoader

        mock_args.data_dir = os.path.dirname(mini_dataset_path)
        mock_args.batch_size = 2
        mock_args.tsteps = 10

        class MockLogger:
            def write(self, msg):
                pass

        data_loader = DataLoader(mock_args, logger=MockLogger())
        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

        @tf.function
        def train_step(inputs, targets):
            with tf.GradientTape() as tape:
                predictions = tiny_model(inputs, training=True)
                loss = compute_loss(predictions, targets)
            gradients = tape.gradient(loss, tiny_model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, tiny_model.trainable_variables))
            return loss

        # Train for 5 steps
        for _ in range(5):
            x, y, s, c = data_loader.next_batch()
            train_inputs = {
                'stroke_data': tf.constant(x, dtype=tf.float32),
                'char_seq': tf.constant(c, dtype=tf.float32)
            }
            train_targets = tf.constant(y, dtype=tf.float32)
            train_step(train_inputs, train_targets)

        # Save checkpoint
        checkpoint = tf.train.Checkpoint(model=tiny_model, optimizer=optimizer)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=1
        )
        manager.save(checkpoint_number=5)

        # Save weights after 5 steps
        weights_step5 = [var.numpy().copy() for var in tiny_model.trainable_variables]

        # Continue training for 5 more steps
        for _ in range(5):
            x, y, s, c = data_loader.next_batch()
            train_inputs = {
                'stroke_data': tf.constant(x, dtype=tf.float32),
                'char_seq': tf.constant(c, dtype=tf.float32)
            }
            train_targets = tf.constant(y, dtype=tf.float32)
            train_step(train_inputs, train_targets)

        # Save weights after 10 steps (continuous training)
        weights_step10_continuous = [var.numpy().copy() for var in tiny_model.trainable_variables]

        # Now simulate resume: reset weights, restore checkpoint, train 5 more
        checkpoint.restore(manager.latest_checkpoint)

        # Verify restored to step 5 state
        for w5, var in zip(weights_step5, tiny_model.trainable_variables):
            np.testing.assert_allclose(w5, var.numpy())

        # Continue training for 5 more steps
        for _ in range(5):
            x, y, s, c = data_loader.next_batch()
            train_inputs = {
                'stroke_data': tf.constant(x, dtype=tf.float32),
                'char_seq': tf.constant(c, dtype=tf.float32)
            }
            train_targets = tf.constant(y, dtype=tf.float32)
            train_step(train_inputs, train_targets)

        # Weights after resumed training
        weights_step10_resumed = [var.numpy().copy() for var in tiny_model.trainable_variables]

        # Should be different from step 5 (training continued)
        for w5, w10 in zip(weights_step5, weights_step10_resumed):
            difference = np.abs(w5 - w10).max()
            assert difference > 0, "Weights should change after resumed training"


@pytest.mark.integration
class TestCheckpointCompatibility:
    """Test checkpoint compatibility and edge cases."""

    def test_checkpoint_with_different_model_partial_restore(self, tiny_model,
                                                             temp_checkpoint_dir,
                                                             reset_random_seeds):
        """Test loading checkpoint with wrong model architecture allows partial restore."""
        # Save checkpoint
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        # Create model with different architecture
        from model import HandwritingModel

        class DifferentArgs:
            rnn_size = 20  # Different size (tiny_model has 10)
            nmixtures = 2
            kmixtures = 1
            alphabet = ' abc'
            tsteps_per_ascii = 25
            dropout = 1.0

        different_model = HandwritingModel(DifferentArgs())

        new_checkpoint = tf.train.Checkpoint(model=different_model)

        # TensorFlow allows partial restore with warnings (doesn't raise exception)
        # Restore completes but some variables may not match
        try:
            status = new_checkpoint.restore(save_path)
            # If we reach here, restore completed without exception
            # TensorFlow handles partial restore gracefully (loads matching variables)
            assert status is not None, "Restore should return status object"
        except Exception as e:
            pytest.fail(f"Partial restore should not raise exception: {e}")

    def test_checkpoint_directory_created(self, tiny_model):
        """Test checkpoint creates directory if it doesn't exist."""
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            checkpoint_dir = os.path.join(tmpdir, 'new_dir', 'checkpoints')

            # Directory doesn't exist yet
            assert not os.path.exists(checkpoint_dir)

            checkpoint = tf.train.Checkpoint(model=tiny_model)
            manager = tf.train.CheckpointManager(
                checkpoint,
                directory=checkpoint_dir,
                max_to_keep=1
            )

            # Save should create directory
            manager.save(checkpoint_number=0)

            # Directory should now exist
            assert os.path.exists(checkpoint_dir)

    def test_checkpoint_with_no_training(self, tiny_model, temp_checkpoint_dir):
        """Test checkpoint can be saved immediately without training."""
        # Save checkpoint without any training
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        save_path = checkpoint.save(os.path.join(temp_checkpoint_dir, 'model'))

        # Should succeed
        assert os.path.exists(save_path + '.index')

        # Restore should work
        new_checkpoint = tf.train.Checkpoint(model=tiny_model)
        status = new_checkpoint.restore(save_path)

        # Should restore successfully
        status.assert_consumed()


@pytest.mark.integration
class TestCheckpointNumbering:
    """Test checkpoint numbering and extraction."""

    def test_extract_step_number_from_checkpoint(self, tiny_model,
                                                 temp_checkpoint_dir):
        """Test step number can be extracted from checkpoint path."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=5
        )

        # Save with specific number
        manager.save(checkpoint_number=42)

        # Extract number from path
        latest = manager.latest_checkpoint
        assert '42' in latest or 'ckpt-42' in latest

        # Try to extract step number
        try:
            step = int(latest.split('-')[-1])
            assert step == 42
        except ValueError:
            pytest.fail("Should be able to extract step number from checkpoint path")

    def test_checkpoint_numbers_increase(self, tiny_model, temp_checkpoint_dir):
        """Test checkpoint numbers increase correctly."""
        checkpoint = tf.train.Checkpoint(model=tiny_model)
        manager = tf.train.CheckpointManager(
            checkpoint,
            directory=temp_checkpoint_dir,
            max_to_keep=10
        )

        # Save checkpoints with increasing numbers
        numbers = [1, 5, 10, 20, 50]
        for num in numbers:
            manager.save(checkpoint_number=num)

        # All checkpoints should exist
        checkpoints = manager.checkpoints
        assert len(checkpoints) == len(numbers)


# ============================================================================
# Summary
# ============================================================================

def test_checkpointing_suite_summary():
    """
    Checkpointing test suite summary.

    If all checkpointing tests pass, save/load works correctly:
    - Checkpoints save to disk
    - CheckpointManager works
    - Model weights restored correctly
    - Optimizer state restored
    - Training can resume from checkpoint
    - Latest checkpoint found automatically
    - Checkpoint numbering works
    - Directory creation works
    """
    print("\nâœ“ All checkpointing tests passed!")
    print("  - Checkpoints save successfully")
    print("  - Model weights restore correctly")
    print("  - Optimizer state restores")
    print("  - Training resumes correctly")
    print("  - CheckpointManager works")
